import torch
import torch.autograd as autograd


def compute_gradient_penalty(critic, real_samples, fake_samples, device):
    """
    Calculates the gradient penalty loss for Wasserstein GAN with gradient penalty (WGAN-GP).
    The gradient penalty encourages the gradients of the discriminator's scores wrt the
    interpolated samples to have a norm of 1. It functions as such:
    1. Randomly interpolate the real and fake samples using epsilon-weighted average.
    2. Call the critic to compute the scores for the interpolated samples
    3. Calculate the gradients of the scores w.r.t. the interpolated samples
    4. Compute the gradient penalty as the mean squared norm of the gradients

    Args:
    :param critic: torch.nn.Module. The critic network.
    :param real_samples: torch.Tensor. The real samples from the data distribution.
                                       Shape (batch_size, C, W, H).
    :param fake_samples: torch.Tensor. The fake samples generated by the generator.
                                       Shape (batch_size, C, W, H).
    :param device: torch.device. The device on which the computations will be performed.

    :returns torch.Tensor. The gradient penalty loss

    """
    batch_size, C, W, H = real_samples.shape
    epsilon = torch.rand(batch_size, 1, 1, 1).repeat(1, C, W, H).to(device)
    interpolated_images = (epsilon * real_samples + ((1 - epsilon) * fake_samples))
    interpolated_scores = critic(interpolated_images)

    # Get gradient w.r.t. interpolated scores
    gradients = autograd.grad(
        inputs=interpolated_images,
        outputs=interpolated_scores,
        grad_outputs=torch.ones_like(interpolated_scores),
        create_graph=True,
        retain_graph=True,
    )[0]
    gradients = gradients.view(gradients.shape[0], -1)
    gradient_penalty = torch.mean((1. - torch.sqrt(1e-8+torch.sum(gradients**2, dim=1)))**2)

    return gradient_penalty
