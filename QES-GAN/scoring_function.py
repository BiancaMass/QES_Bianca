# Critic Network: You should have a critic network that takes an image as input and outputs a
# scalar value. This network is trained to approximate the Wasserstein distance. For real
# images, it should output higher values; for generated images, lower values.

# THE CRITIC CAN BE TRAINED ONLY ONCE AND KEPT GOOD! FOR EVALUATION OF NEW ANSATZES - HOWEVER
# THERE ARE SOME RISKS WITH THIS APPROACH:
# 1. risking overfiting the generator to whatever critic you are training to, making the result of
# the evolution less generalizable
# 2. Using a fixed critic for evaluation, means having a consistent benchmark, allowing to compare
# different ansatzes architectures under the same standards. However, this approach assumes the
# critic is well-trained and can generalize across a wide variety of generated images. The critic
# needs to be trained on a diverse set of images and should be robust enough to handle a wide
# range of generator outputs
# 3. As a solution, you could consider starting with a pretty solid critic and and re-training it
# every so often, especially if you notice signs of overfitting, such as stagnation in the
# evolutionary processes, discrepancies between scores and visual quality. It is also good
# practice to validate the  generators with a separate set of real images that the critic has not
# been trained on.
#
# Scoring Function: The scoring function will take an 'organism' (in this case, an image
# generated by the GAN's generator) and pass it through the critic network to get the Wasserstein
# distance.


def scoring_function(ansatz, critic):
    # Generate a batch of images using the current organism (ansatz)

    # Generate a latent vector here or give it as input to the function?
    latent_vector = []
    generated_images = ansatz(latent_vector)

    # Evaluate these images using the trained critic network
    scores = critic(generated_images)

    # Calculate the average score for this batch as the organism's score
    average_score = scores.mean().item()

    return average_score
